import streamlit as st
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus
from langchain_community.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate
from pymilvus import connections, utility

# Connexion Milvus
connections.connect(
   alias="default",
   host="localhost", 
   port="19530"
)

# Configuration
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
collection_name = "planet_mer"
db = Milvus(
   embedding_function=embedding_model,
   collection_name=collection_name,
   connection_args={"host": "localhost", "port": "19530"}
)

retriever = db.as_retriever(
   search_type="similarity",
   search_kwargs={"k": 2}
)

# LLM Config
llm = ChatOllama(model="llama3.2", keep_alive="3h", max_tokens=512, temperature=0.1)

# Tests de v√©rification
print("Collection existe:", utility.has_collection("planet_mer"))
print("Test requ√™te:", len(retriever.get_relevant_documents("test")))
print("Nombre documents:", db.col.num_entities)

def format_docs(docs):
   formatted_docs = [doc.page_content for doc in docs]
   sources = [doc.metadata.get('source', 'Source inconnue') for doc in docs]
   return "\n\n".join(formatted_docs), sources

def generate_response_with_sources(retriever, question):
   docs = retriever.get_relevant_documents(question)
   context, sources = format_docs(docs)
   
   messages = [
    {"role": "system", "content": "Vous √™tes un expert en biologie marine et en p√™che. Vous devez r√©pondre de mani√®re pr√©cise sur les esp√®ces marines r√©glement√©es en M√©diterran√©e fran√ßaise."},
    {"role": "user", "content": "Pouvez-vous me donner une liste des esp√®ces marines r√©glement√©es en M√©diterran√©e, y compris la dorade rose, le thon germon, et d'autres esp√®ces mentionn√©es dans les r√®glements halieutiques ?"},
       {"role": "user", "content": f"Context: {context}\n\nQuestion: {question}"}
   ]
   
   chain_response = llm.invoke(messages)
   return chain_response.content, sources

# Streamlit UI
st.set_page_config(page_title="Plan√®te Mer ChatBot", page_icon="üê†")

st.markdown("""
   <style>
       .user-message {
           background-color: #e3f2fd;
           color: #0d47a1;
           padding: 10px;
           border-radius: 10px;
           margin: 5px 0;
           max-width: 80%;
           align-self: flex-end;
       }
       .assistant-message {
           background-color: #f1f8e9;
           color: #33691e;
           padding: 10px;
           border-radius: 10px;
           margin: 5px 0;
           max-width: 80%;
           align-self: flex-start;
       }
   </style>
""", unsafe_allow_html=True)

if "messages" not in st.session_state:
   st.session_state.messages = [
       {"role": "assistant", "content": ("Bonjour! Je suis l√† pour r√©pondre √† vos questions sur la p√™che et la vie marine. Comment puis-je vous aider? üé£", [])}
   ]

with st.sidebar:
   st.title("ChatBot Plan√®te Mer")
   if st.button("Effacer l'historique"):
       st.session_state.messages = [
           {"role": "assistant", "content": ("Bonjour! Je suis l√† pour r√©pondre √† vos questions sur la p√™che et la vie marine. Comment puis-je vous aider? üé£", [])}
       ]
   
   st.markdown("### Historique des conversations")
   for message in st.session_state.messages[1:]:
       if message["role"] == "user":
           st.markdown(f"**üòé Vous:** {message['content'][:100]}...")
       else:
           response, _ = message["content"]
           st.markdown(f"**ü§ñ Assistant:** {response[:100]}...")

st.title("Bonjour !")

for message in st.session_state.messages:
   if message["role"] == "user":
       st.markdown(f'<div class="user-message">{message["content"]}</div>', unsafe_allow_html=True)
   else:
       response, sources = message["content"]
       st.markdown(f'<div class="assistant-message">{response}</div>', unsafe_allow_html=True)
       if sources:
           with st.expander("Sources"):
               for source in sources:
                   st.markdown(f"- {source}")

if prompt := st.chat_input("Posez votre question ici..."):
   st.session_state.messages.append({"role": "user", "content": prompt})
   st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)
   
   with st.spinner("R√©flexion en cours..."):
       response, sources = generate_response_with_sources(retriever, prompt)
       st.session_state.messages.append({"role": "assistant", "content": (response, sources)})
       st.markdown(f'<div class="assistant-message">{response}</div>', unsafe_allow_html=True)
       if sources:
           with st.expander("Sources"):
               for source in sources:
                   st.markdown(f"- {source}")